# lab_10
Aashika Uppala
4/22/2025

Introduction
The objective of this lab was to use Python and AI tools to develop user-friendly applications that search and visualize data from large water quality databases provided by USGS.
Students used the Pandas Python library and a Large Language Model (LLM) to generate functions that cleaned and searched station and results databases. Part 1 involved mapping water quality stations using location data. Part 2 required plotting specific contaminant trends over time across various sites. Part 3 focused on creating an interactive web app using Streamlit that allows users to upload datasets, filter by contaminants, date range, and value, and visualize results dynamically.
The final outcome was a fully functional Streamlit web application that dynamically filters and displays water quality station locations and contaminant trends. Python scripts were developed collaboratively with the help of AI tools and deployed via GitHub and Streamlit.

Methods/Tests
To develop scripts and an app capable of parsing and visualizing water quality data by site and contaminant trends.
AI Models Used:
ChatGPT (GPT-4), OpenAI, 2025

Results
Tables:
Prompts:
Link to repository:
Link to app: https://laughing-rotary-phone-r46wx6q96gw7c5w5q.github.dev/

Discussion 
This lab demonstrated the power of combining AI tools with programming to solve real-world data challenges. By leveraging LLM-generated code, we were able to quickly build and debug functions for parsing and visualizing large datasets. We also learned the importance of prompt iteration and specificity when collaborating with AI models. The Streamlit app showcases how code can be converted into a practical tool for scientific communication and public accessibility.

Conclusion
This lab reinforced practical skills in data analysis using Pandas and introduced web deployment via Streamlit. Key takeaways include:
Effective data cleaning and filtering using Python; Visualizing trends to derive meaningful insights; Learning how to prompt AI to write efficient, modular code;
Creating and deploying real-time applications from codebases; These tools and skills have broad implications for environmental monitoring, research communication, and data-driven policy decisions.
